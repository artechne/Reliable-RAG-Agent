{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import get_openai_api_key, get_serper_api_key\n",
    "\n",
    "OPENAI_MODEL_NAME = 'gpt-4-turbo'\n",
    "\n",
    "openai_api_key = get_openai_api_key()\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = OPENAI_MODEL_NAME\n",
    "os.environ[\"SERPER_API_KEY\"] = get_serper_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document load\n",
    "from populate_db import load_documents, split_documents\n",
    "\n",
    "docs = load_documents()\n",
    "print(f\"Length of docs : {len(docs)}\")\n",
    "#print(docs[0])\n",
    "\n",
    "# Split documents\n",
    "chunks = split_documents(docs)\n",
    "print(f\"Length of chunks : {len(chunks)}\")\n",
    "#print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update CHROMA DB if need to be updated\n",
    "from populate_db import add_to_chroma_db, clear_chroma_db\n",
    "\n",
    "#clear_chroma_db()\n",
    "add_to_chroma_db(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Test not using Agent, just Query\n",
    "from custom_rag_tool import query_rag\n",
    "from IPython.display import Markdown\n",
    "\n",
    "query_response = query_rag(\"5차 회의에서는 무슨 주제로 무제한 토론을 진행했나요?\")\n",
    "\n",
    "Markdown(query_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai_tools import SerperDevTool\n",
    "from custom_rag_tool import CustomChromaDBRagTool\n",
    "\n",
    "web_search_tool = SerperDevTool()\n",
    "rag_tool = CustomChromaDBRagTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_agent = Agent(\n",
    "    role=\"Retriever\",\n",
    "    goal=\"Retrieve the relevaent information from the vectorstore to answer the question\",\n",
    "    backstory=(\n",
    "        \"You are an assistant for question-answering tasks.\"\n",
    "        \"Use the information present in the retrieved context to answer the question.\"\n",
    "        \"You have to provide a clear concise answer.\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "retriever_task = Task(\n",
    "    description=(\n",
    "        \"Use the rag_tool, extract information for the question {question}.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Set of relevant documents and passages that can be used to answer the question {question}.\"\n",
    "        \"Return a clear and consise text as response.\"\n",
    "        \"Please give all answers in Korean.\"\n",
    "    ),\n",
    "    agent=retriever_agent,\n",
    "    tools=[rag_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_agent =  Agent(\n",
    "  role='Answer Grader',\n",
    "  goal='Filter out erroneous retrievals',\n",
    "  backstory=(\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
    "    \"If the document contains keywords related to the user question, grade it as relevant.\"\n",
    "    \"It does not need to be a stringent test.You have to make sure that the answer is relevant to the question.\"\n",
    "  ),\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    ")\n",
    "\n",
    "grader_task = Task(\n",
    "    description=(\n",
    "        \"Based on the response from the retriever task for the quetion {question} evaluate whether the retrieved content is relevant to the question.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\"\n",
    "        \"You must answer 'yes' if the response from the 'retriever_task' is in alignment with the question asked.\"\n",
    "        \"You must answer 'no' if the response from the 'retriever_task' is not in alignment with the question asked.\"\n",
    "        \"Please provide explanation about the answer.\"\n",
    "        \"Please give all answers in Korean.\"\n",
    "        #\"Do not provide any preamble or explanations except for 'yes' or 'no'.\"\n",
    "    ),\n",
    "    agent=grader_agent,\n",
    "    context=[retriever_task],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_grader = Agent(\n",
    "    role=\"Hallucination Grader\",\n",
    "    goal=\"Filter out hallucination\",\n",
    "    backstory=(\n",
    "        \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n",
    "        \"Make sure you meticulously review the answer and check if the response provided is in alignmnet with the question asked\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "hallucination_task = Task(\n",
    "    description=(\n",
    "        \"Based on the response from the grader task for the quetion {question} evaluate whether the answer is grounded in / supported by a set of facts.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Binary score 'yes' or 'no' score to indicate whether the answer is sync with the question asked\"\n",
    "        \"Respond 'yes' if the answer is in useful and contains fact about the question asked.\"\n",
    "        \"Respond 'no' if the answer is not useful and does not contains fact about the question asked.\"\n",
    "        \"Please provide explanation about the answer.\"\n",
    "        \"Please give all answers in Korean.\"\n",
    "        #\"Do not provide any preamble or explanations except for 'yes' or 'no'.\"\n",
    "    ),\n",
    "    agent=hallucination_grader,\n",
    "    context=[grader_task],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_grader = Agent(\n",
    "    role=\"Answer Grader\",\n",
    "    goal=\"Filter out hallucination from the answer.\",\n",
    "    backstory=(\n",
    "        \"You are a grader assessing whether an answer is useful to resolve a question.\"\n",
    "        \"Make sure you meticulously review the answer and check if it makes sense for the question asked\"\n",
    "        \"If the answer is relevant then generate a clear and concise response.\"\n",
    "        \"If the answer gnerated is not relevant then perform a websearch using 'web_search_tool'\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "answer_task = Task( \n",
    "    description=(\n",
    "        \"Based on the response from the hallucination task for the quetion {question} evaluate whether the answer is useful to resolve the question.\"\n",
    "        \"If the answer is 'yes' return a clear and concise answer with Korean.\"\n",
    "        \"If the answer is 'no' then perform a 'websearch' and return the response\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Return a clear and concise response if the response from 'hallucination_task' is 'yes'.\"\n",
    "        \"Perform a web search using 'web_search_tool' and return ta clear and concise response only if the response from 'hallucination_task' is 'no'.\"\n",
    "        \"Otherwise respond as 'Sorry! unable to find a valid response'.\"\n",
    "        \"Please give all answers in Korean.\"\n",
    "    ),  \n",
    "    context=[hallucination_task],\n",
    "    agent=answer_grader,\n",
    "    tools=[web_search_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_crew = Crew(\n",
    "    agents=[retriever_agent, grader_agent, hallucination_grader, answer_grader],\n",
    "    tasks=[retriever_task, grader_task, hallucination_task, answer_task],\n",
    "    memory=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\":\"대한민국 22대 415회 국회본회의 5차 회의에서는 무슨 주제로 무제한 토론을 진행했나요?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_crew.kickoff(inputs=inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rraucai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
